{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "read_df = pd.read_csv('Fire_Incidents.tsv', sep='\\t')\n",
    "\n",
    "read_df.shape\n",
    "\n",
    "def delete_columns(col):\n",
    "    if read_df[col].isnull().sum() > read_df[col].count()/2:\n",
    "        del read_df[col]\n",
    "for col in read_df.columns:\n",
    "    delete_columns(col)\n",
    "\t\n",
    "\t\n",
    "read_df.head()\n",
    "\n",
    "\n",
    "df_X1 = read_df.interpolate(method ='nearest') \n",
    "\n",
    "\n",
    "def Property_Use_Conv(read_df):\n",
    "        temp = str(read_df)\n",
    "        temp = temp.strip()\n",
    "        #temp = re.sub(r'\\d+',temp )\n",
    "        return temp[:3]\n",
    "\t\t\n",
    "df_X1['Action Taken Primary'] = df_X1['Action Taken Primary'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X1['Action Taken Primary'] = df_X1['Action Taken Primary'].apply(lambda s: 86 if str(s) == '-' or str(s).startswith('Nan') or str(s).startswith('nan') else s)\n",
    "\n",
    "\n",
    "df_X1['Action Taken Secondary'] = df_X1['Action Taken Secondary'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X1['Action Taken Secondary'] = df_X1['Action Taken Secondary'].apply(lambda s: 86 if str(s) == '-' or str(s).startswith('Nan') or str(s).startswith('nan') else s)\n",
    "\n",
    "\n",
    "df_X1['Action Taken Other'] = df_X1['Action Taken Other'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X1['Action Taken Other'] = df_X1['Action Taken Other'].apply(lambda s: 86 if str(s) == '-' or str(s).startswith('Nan') or str(s).startswith('nan') else s)\n",
    "\n",
    "\n",
    "df_X1['Detector Alerted Occupants'] = df_X1['Detector Alerted Occupants'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X1['Detector Alerted Occupants'] = df_X1['Detector Alerted Occupants'].apply(lambda s: 0 if str(s) == '-' or str(s).startswith('u') or str(s).startswith('nan') else s)\n",
    "\n",
    "df_X1['Station Area'] = df_X1['Station Area'].apply(lambda s: 40 if str(s).startswith('A') or str(s).startswith('H') or str(s).startswith('O') or str(s).startswith('nan') or str(s) == '-' else s)\n",
    "\n",
    "df_X1['Action Taken Secondary'] = df_X1['Action Taken Secondary'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 or str(s).find('*') != -1 or str(s).find('d') != -1 else s)  \n",
    "df_X1['Action Taken Primary'] = df_X1['Action Taken Primary'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 or str(s).find('*') != -1 or str(s).find('d') != -1 else s)  \n",
    "df_X1['Action Taken Other'] = df_X1['Action Taken Other'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 or str(s).find('*') != -1 or str(s).find('d') != -1 else s)  \n",
    "df_X1['Detector Alerted Occupants'] = df_X1['Detector Alerted Occupants'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 or str(s).find('*') != -1 or str(s).find('d') != -1 else s)  \n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_date_to_weekday(tmstp):\n",
    "    result = datetime.strptime(tmstp, '%m/%d/%Y %I:%M:%S %p').weekday()\n",
    "    return result\n",
    "    \n",
    "def convert_date_to_hour(tmstp):\n",
    "    result = datetime.strptime(tmstp, '%m/%d/%Y %I:%M:%S %p').hour\n",
    "    return result\n",
    "\n",
    "def convert_date_to_month(tmstp):\n",
    "    result = datetime.strptime(tmstp, '%m/%d/%Y %I:%M:%S %p').month\n",
    "    return result\n",
    "\t\n",
    "df_X1['Weekday'] = df_X1['Alarm DtTm'].apply(lambda x: convert_date_to_weekday(x))\n",
    "df_X1['Hour'] = df_X1['Alarm DtTm'].apply(lambda x: convert_date_to_hour(x))\n",
    "df_X1['Month'] = df_X1['Alarm DtTm'].apply(lambda x: convert_date_to_month(x))\n",
    "\n",
    "\n",
    "def missing_values():\n",
    "    temp_dict = dict()\n",
    "    for i in df_X1.columns:\n",
    "        if df_X1[i].isnull().sum() > 0: \n",
    "            temp_dict[i] = df_X1[i].isnull().sum()\n",
    "    return temp_dict\n",
    "\t\n",
    "\t\n",
    "missing_values()\n",
    "\n",
    "for col in df_X1.columns:\n",
    "    df_X1[col] = df_X1[col].fillna(method='bfill')\n",
    "\n",
    "\t\n",
    "df_X1 = df_X1.drop('First Unit On Scene', axis = 1)\n",
    "\n",
    "df_X1 = df_X1.drop(['Alarm DtTm', 'Arrival DtTm', 'Close DtTm'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "df_X1['Battalion'] = pd.Categorical(df_X1['Battalion'])\n",
    "one_hot = pd.get_dummies(df_X1['Battalion'],prefix='Battalion')\n",
    "    #df_X2 = df_X2.drop('Battalion',axis = 1)\n",
    "    # Join the encoded df\n",
    "df_X1 = df_X1.join(one_hot)\n",
    "df_X1['Zipcode'] = pd.Categorical(df_X1['Zipcode'])\n",
    "one_hot = pd.get_dummies(df_X1['Zipcode'],prefix='Zipcode')\n",
    "    #df_X2 = df_X2.drop('Battalion',axis = 1)\n",
    "    # Join the encoded df\n",
    "df_X1 = df_X1.join(one_hot)\n",
    "df_feature  = df_X1.drop(['Battalion','Zipcode'],axis=1)\n",
    "\n",
    "\n",
    "df_y = df_feature['Suppression Personnel']\n",
    "df_feature = df_feature.drop(['Suppression Personnel'],axis=1)\n",
    "\n",
    "\n",
    "# df_X1_mini = df_feature[:100]\n",
    "\n",
    "\n",
    "# df_y = df_X1_mini['Suppression Personnel']\n",
    "# df_X1_mini = df_X1_mini.drop(['Suppression Personnel'],axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def dataframetoCSRmatrix(df):\n",
    "    nrows = len(df)\n",
    "    nc = len(df.columns)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = nc * nrows\n",
    "    \n",
    "    cols= df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(str)\n",
    "        for name in df[col].unique():\n",
    "            idx[col+name] = tid\n",
    "            tid += 1\n",
    "    \n",
    "    ncols = len(idx)\n",
    "    \n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.int)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    \n",
    "    i=0\n",
    "    n=0\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        for j,col in enumerate(cols):\n",
    "            ind[j+n] = idx[col+row[col]]\n",
    "            val[j+n] = 1\n",
    "        ptr[i+1] = ptr[i] + nc\n",
    "        n += nc\n",
    "        i += 1\n",
    "    \n",
    "    mat = csr_matrix((val,ind,ptr), shape=(nrows,ncols), dtype=np.int)\n",
    "    mat.sort_indices()   \n",
    "    \n",
    "    return mat\n",
    "    \n",
    "mat1 = dataframetoCSRmatrix(df_feature)\n",
    "\n",
    "\n",
    "mat1.shape\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_feature_train, df_feature_test, df_y_train, df_y_test = train_test_split(mat1, df_y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rfreg = RandomForestRegressor(n_estimators= 10,max_depth=1, random_state=42)\n",
    "# rfreg.fit(df_feature_train, df_y_train)\n",
    "\n",
    "\n",
    "# predicted_y_rf = rfreg.predict(df_feature_test)\n",
    "\n",
    "\n",
    "# set(predicted_y_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "# print(\"r2_score: \",r2_score(df_y_test, predicted_y_rf))\n",
    "# print(\"explained_variance_score: \",explained_variance_score(df_y_test, predicted_y_rf))\n",
    "# print(\"mean_absolute_error: \",mean_absolute_error(df_y_test, predicted_y_rf))\n",
    "# print(\"mean_squared_error: \",mean_squared_error(df_y_test, predicted_y_rf))\n",
    "# print(\"median_absolute_error: \",median_absolute_error(df_y_test, predicted_y_rf))\n",
    "# print(\"mean_squared_log_error: \",mean_squared_log_error(df_y_test, predicted_y_rf))\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def lasso(x_train, x_validation,  y_train, y_validation):\n",
    "    reg = linear_model.Lasso(alpha = 0.1)\n",
    "    # X = np.array(X).reshape([-1, 1])\n",
    "    reg.fit(x_train,y_train)\n",
    "    y_pred_list = reg.predict(x_validation)\n",
    "    mse = mean_squared_error(y_validation, y_pred_list)\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=34234)\n",
    "    cross_val_scores = cross_val_score(reg, x_train, y_train, cv=kfold)\n",
    "    print(\"\\nLasso Regression Model\")\n",
    "    print(cross_val_scores, mse)\n",
    "    return cross_val_scores, mse\n",
    "\n",
    "\n",
    "#lasso(X_train,X_test, df_y_train, df_y_test)\n",
    "lasso(df_feature_train,df_feature_test, df_y_train, df_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
