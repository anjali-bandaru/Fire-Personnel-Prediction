{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different trial and error methods that we tried during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Fire_Incidents.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Incident Number','Address','Call Number', 'Arrival DtTm', 'Close DtTm', 'Suppression Units', 'EMS Units', 'EMS Personnel', 'Other Units', 'Other Personnel', 'First Unit On Scene', 'Estimated Property Loss', \n",
    "         'Estimated Contents Loss', 'Fire Fatalities', 'Fire Injuries', 'Civilian Fatalities', 'Civilian Injuries', \n",
    "         'Mutual Aid', 'Action Taken Secondary', 'Action Taken Other', 'Area of Fire Origin', \n",
    "         'Ignition Factor Primary', 'Ignition Factor Secondary', 'Heat Source','Item First Ignited',\n",
    "         'Human Factors Associated with Ignition', 'Floor of Fire Origin', 'Fire Spread','No Flame Spead',\n",
    "         'Number of floors with minimum damage','Number of floors with significant damage','Number of floors with heavy damage',\n",
    "         'Number of floors with extreme damage','Detector Type','Detector Operation','Detector Effectiveness',\n",
    "         'Detector Failure Reason','Automatic Extinguishing System Present','Automatic Extinguishing Sytem Type',\n",
    "         'Automatic Extinguishing Sytem Perfomance','Automatic Extinguishing Sytem Failure Reason','Location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with more than half null values\n",
    "def delete_columns(df, col):\n",
    "    if df[col].isnull().sum() > df[col].count()/2:\n",
    "        del df[col]\n",
    "        \n",
    "for col in df.columns:\n",
    "    delete_columns(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.interpolate(method ='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Property_Use_Conv(df):\n",
    "    temp = str(df)\n",
    "    temp = temp.strip()\n",
    "    #temp = re.sub(r'\\d+',temp )\n",
    "    return temp[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X['Property Use'] = df_X['Property Use'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X['Property Use'] = df_X['Property Use'].apply(lambda s: 500 if str(s) == 'uuu' or str(s) == 'nnn' or str(s) == 'nan' or str(s) == '-' else s)\n",
    "df_X['Property Use'] = df_X['Property Use'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 else s)\n",
    "df_X['Primary Situation'] = df_X['Primary Situation'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X['Primary Situation'] = df_X['Primary Situation'].apply(lambda s: 650 if str(s) == 'n/a' or str(s).startswith('cr') or str(s).startswith('y') or str(s) == '-' else s)\n",
    "df_X['Primary Situation'] = df_X['Primary Situation'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 or str(s).find('*') != -1 else s)  \n",
    "df_X['Action Taken Primary'] = df_X['Action Taken Primary'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X['Action Taken Primary'] = df_X['Action Taken Primary'].apply(lambda s: 86 if str(s) == '-' or str(s).startswith('Nan') or str(s).startswith('nan') else s)\n",
    "df_X['Detector Alerted Occupants'] = df_X['Detector Alerted Occupants'].apply(lambda s: Property_Use_Conv(s))\n",
    "df_X['Detector Alerted Occupants'] = df_X['Detector Alerted Occupants'].apply(lambda s: 0 if str(s) == '-' or str(s).startswith('u') or str(s).startswith('nan') else s)\n",
    "df_X['Station Area'] = df_X['Station Area'].apply(lambda s: 40 if str(s).startswith('A') or str(s).startswith('H') or str(s).startswith('O') or str(s).startswith('nan') or str(s) == '-' else s)\n",
    "df_X['Detector Alerted Occupants'] = df_X['Detector Alerted Occupants'].apply(lambda s: str(s)[0] if str(s).find('-') != -1 or str(s).find('*') != -1 or str(s).find('d') != -1 else s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_date_to_weekday(tmstp):\n",
    "    result = datetime.strptime(tmstp, '%m/%d/%Y %I:%M:%S %p').weekday()\n",
    "    return result\n",
    "    \n",
    "def convert_date_to_hour(tmstp):\n",
    "    result = datetime.strptime(tmstp, '%m/%d/%Y %I:%M:%S %p').hour\n",
    "    return result\n",
    "\n",
    "def convert_date_to_month(tmstp):\n",
    "    result = datetime.strptime(tmstp, '%m/%d/%Y %I:%M:%S %p').month\n",
    "    return result\n",
    "\n",
    "df_X['Weekday'] = df_X['Alarm DtTm'].apply(lambda x: convert_date_to_weekday(x))\n",
    "df_X['Hour'] = df_X['Alarm DtTm'].apply(lambda x: convert_date_to_hour(x))\n",
    "df_X['Month'] = df_X['Alarm DtTm'].apply(lambda x: convert_date_to_month(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Y = df_X['Suppression Personnel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values():\n",
    "    temp_dict = dict()\n",
    "    for i in df_X.columns:\n",
    "        if df_X[i].isnull().sum() > 0: \n",
    "            temp_dict[i] = df_X[i].isnull().sum()\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_values()\n",
    "\n",
    "for col in df_X.columns:\n",
    "    df_X[col] = df_X[col].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X['Battalion'] = pd.Categorical(df_X['Battalion'])\n",
    "one_hot = pd.get_dummies(df_X['Battalion'],prefix='Battalion')\n",
    "    #df_X2 = df_X2.drop('Battalion',axis = 1)\n",
    "    # Join the encoded df\n",
    "df_X = df_X.join(one_hot)\n",
    "df_X['Zipcode'] = pd.Categorical(df_X['Zipcode'])\n",
    "one_hot = pd.get_dummies(df_X['Zipcode'],prefix='Zipcode')\n",
    "    #df_X2 = df_X2.drop('Battalion',axis = 1)\n",
    "    # Join the encoded df\n",
    "df_X = df_X.join(one_hot)\n",
    "df_feature  = df_X.drop(['Battalion','Zipcode'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature.drop(['Alarm DtTm','City','Incident Date','Neighborhood  District'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = df_feature[df_feature['Suppression Personnel'] <= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df['Suppression Personnel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = filter_df['Suppression Personnel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = filter_df.drop(['Suppression Personnel'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_X1_mini = df_feature[:50000]\n",
    "# df_y_mini = df_Y[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframetoCSRmatrix(df):\n",
    "    nrows = len(df)\n",
    "    nc = len(df.columns)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = nc * nrows\n",
    "    \n",
    "    cols= df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(str)\n",
    "        for name in df[col].unique():\n",
    "            idx[col+name] = tid\n",
    "            tid += 1\n",
    "    \n",
    "    ncols = len(idx)\n",
    "    \n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.int)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    \n",
    "    i=0\n",
    "    n=0\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        for j,col in enumerate(cols):\n",
    "            ind[j+n] = idx[col+row[col]]\n",
    "            val[j+n] = 1\n",
    "        ptr[i+1] = ptr[i] + nc\n",
    "        n += nc\n",
    "        i += 1\n",
    "    \n",
    "    mat = csr_matrix((val,ind,ptr), shape=(nrows,ncols), dtype=np.int)\n",
    "    mat.sort_indices()   \n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = dataframetoCSRmatrix(filter_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# svd = TruncatedSVD(n_components=30, n_iter=10, random_state=42)\n",
    "# reduced_mat = svd.fit_transform(mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_train, df_feature_test, df_y_train, df_y_test = train_test_split(mat1, df_Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# rfreg = RandomForestRegressor(n_estimators= 10,max_depth=1, random_state=42)\n",
    "# rfreg.fit(df_feature_train, df_y_train)\n",
    "\n",
    "\n",
    "# predicted_y_rf = rfreg.predict(df_feature_test)\n",
    "\n",
    "\n",
    "# set(predicted_y_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbr = GradientBoostingRegressor(n_estimators=175, learning_rate=0.08, max_depth=3, random_state=1232, loss='ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_y_rf = rfreg.predict(df_feature_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Y.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rfreg = RandomForestRegressor(n_estimators= 10,max_depth=1, random_state=42)\n",
    "# rfreg.fit(df_feature_train, df_y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_y_rf = rfreg.predict(df_feature_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(predicted_y_rf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# svd = TruncatedSVD(n_components=10, n_iter=10, random_state=42)\n",
    "# reduced_mat = svd.fit_transform(mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_Ridge = Ridge(alpha = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_Ridge.fit(df_feature_train, df_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list_Ridge = reg_Ridge.predict(df_feature_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set(y_pred_list_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r2_score:_ Ridge\",r2_score(df_y_test, y_pred_list_Ridge))\n",
    "print(\"explained_variance_score:Ridge \",explained_variance_score(df_y_test, y_pred_list_Ridge))\n",
    "print(\"mean_absolute_error:Ridge \",mean_absolute_error(df_y_test, y_pred_list_Ridge))\n",
    "print(\"mean_squared_error:Ridge \",mean_squared_error(df_y_test, y_pred_list_Ridge))\n",
    "print(\"median_absolute_error:Ridge \",median_absolute_error(df_y_test, y_pred_list_Ridge))\n",
    "# print(\"mean_squared_log_error:Ridge \",mean_squared_log_error(df_y_test, y_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"r2_score:_ GradientDescent\",r2_score(df_y_test, predicted_y_rf))\n",
    "# print(\"explained_variance_score:_GradientDescent \",explained_variance_score(df_y_test, predicted_y_rf))\n",
    "# print(\"mean_absolute_error:_GradientDescent \",mean_absolute_error(df_y_test, predicted_y_rf))\n",
    "# print(\"mean_squared_error:_GradientDescent \",mean_squared_error(df_y_test, predicted_y_rf))\n",
    "# print(\"median_absolute_error:_GradientDescent \",median_absolute_error(df_y_test, predicted_y_rf))\n",
    "# print(\"mean_squared_log_error:_GradientDescent \",mean_squared_log_error(df_y_test, predicted_y_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(reg_Ridge, 'sf_reg_ridge.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
